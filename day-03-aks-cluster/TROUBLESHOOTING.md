ğŸ§ª Simulating a Broken Service (Deep Dive)

ğŸ”¥ Scenario 1: Service Exists, But Traffic Fails (Label Mismatch)
ğŸ¯ Goal: Understand how Services find Pods.

## Step 1 â€” Working state (mental model)

# Normally, this works:
```yaml
Service selector:
  app: nginx

Pod label:
  app: nginx
```

# Traffic flow:
```**nginx**
Service â†’ matching Pods â†’ response
```
## Step 2 â€” Break it (change selector)

Edit your Service and intentionally mismatch labels:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb
spec:
  type: LoadBalancer
  selector:
    app: wronglabel
  ports:
    - port: 80
      targetPort: 80
```

Apply:
```bash
kubectl apply -f service-loadbalancer.yaml
```
## Step 3 â€” Observe failure
```bash
kubectl get endpoints nginx-lb
```

Output:

<none>

Browser:
âŒ hangs or times out

ğŸ§  Why this broke ?

*Services do NOT talk to Deployments
Services ONLY care about Pod labels
No matching Pods â†’ no endpoints â†’ black hole*

## Step 4 â€” Fix it

Restore selector:
```yaml
selector:
  app: nginx
```

Reapply:
```bash
kubectl apply -f service-loadbalancer.yaml
```

Endpoints reappear:
```bash
kubectl get endpoints nginx-lb
```

Browser:
âœ… works again

ğŸ’¡ Lesson 1

If Service is broken â†’ check labels first

*This is the #1 real-world Service bug.*

ğŸ”¥ Scenario 2: Pods Are Running, But App Is Down (Port Mismatch)
ğŸ¯ Goal: Understand targetPort vs containerPort

## Step 1 â€” Break the Service port

Change Service:
```yaml
ports:
  - port: 80
    targetPort: 9999
```

But container runs on:
```yaml
containerPort: 80
```

Apply:
```bash
kubectl apply -f service-loadbalancer.yaml
```
## Step 2 â€” Observe
```bash
kubectl get pods
```

Pods are:

Running


But browser:
âŒ connection refused / timeout

ğŸ§  Why this broke ?

Traffic flow:

LoadBalancer â†’ Service â†’ Pod:9999 âŒ


Nothing listening on port 9999.

## Step 3 â€” Fix it
targetPort: 80


Reapply â†’ browser works.

ğŸ’¡ Lesson 2

Kubernetes wonâ€™t validate ports for you
Running Pods â‰  Working Service

ğŸ”¥ Scenario 3: Service Is Fine, Pods Keep Restarting
ğŸ¯ Goal: Understand app-level vs service-level failures

## Step 1 â€” Break the container

Change Deployment image to something invalid:

kubectl set image deployment/nginx-deploy nginx=nginx:doesnotexist

## Step 2 â€” Observe
kubectl get pods


Youâ€™ll see:

ImagePullBackOff
CrashLoopBackOff


Service still exists:
```bash
kubectl get svc
```

But browser:
âŒ fails

ğŸ§  Why this broke ?

Service is healthy

But no healthy Pods

Kubernetes refuses to route traffic

Check:
```bash
kubectl get endpoints nginx-lb
```

Empty again.

## Step 3 â€” Fix it
```bash
kubectl set image deployment/nginx-deploy nginx=nginx
```

Pods recover â†’ endpoints return â†’ browser works.

ğŸ’¡ Lesson 3

Services donâ€™t fix broken apps
They only route to healthy Pods

ğŸ§  Debugging Order (VERY IMPORTANT)

When a Service is broken, always debug in this order:

1ï¸âƒ£ Pods running?
```bash
kubectl get pods
```

2ï¸âƒ£ Labels match?
```bash
kubectl describe svc <service>
```

3ï¸âƒ£ Endpoints exist?
```bash
kubectl get endpoints <service>
```

4ï¸âƒ£ Ports correct?
targetPort
containerPort

5ï¸âƒ£ App logs?
```bash
kubectl logs <pod>
```

This order saves hours in real jobs.

**ğŸ”¥ One Golden Rule (memorize)**

Service problems are almost never Service problems
They are usually label, port, or pod health problems