üü¢ Beginner Level (Foundational Understanding)
## Q1. What is monitoring in Kubernetes?

Answer:
Monitoring in Kubernetes is the process of collecting and analyzing metrics about cluster health, resource usage, and application performance to ensure reliability and detect issues early.

## Q2. What are metrics?

Answer:
Metrics are numerical measurements collected over time, such as CPU usage, memory consumption, request rate, and latency.

## Q3. What is Prometheus?

Answer:
Prometheus is a time-series monitoring system that scrapes metrics from targets, stores them, and allows querying and alerting based on those metrics.

## Q4. What is Grafana?

Answer:
Grafana is a visualization tool that queries data sources like Prometheus and displays metrics using dashboards and graphs.

## Q5. What is the difference between Prometheus and Grafana?

Answer:
Prometheus collects and stores metrics, while Grafana visualizes those metrics. Grafana does not store data itself.

üü° Intermediate Level (Operational Knowledge)
## Q6. How does Prometheus collect metrics?

Answer:
Prometheus uses a pull-based model, periodically scraping metrics endpoints exposed by applications or Kubernetes components.

## Q7. What is the Metrics Server? Is it the same as Prometheus?

Answer:
No. Metrics Server provides lightweight, short-term metrics for autoscaling and kubectl top. Prometheus is a full monitoring system used for dashboards and alerts.

## Q8. Why are CPU and memory metrics always visible?

Answer:
Because CPU and memory usage are tracked at the container runtime (cgroup) level and exist even when applications are idle.

## Q9. Why do some Grafana panels show ‚ÄúNo Data‚Äù?

Answer:
Grafana only shows metrics when activity exists. If there is no network traffic, disk I/O, or requests, related panels remain empty.

## Q10. Why were network metrics empty in a test cluster?

Answer:
Because there was minimal or no sustained traffic through Kubernetes Services or ingress, resulting in negligible network activity.

## Q11. Why did a load-generator pod not appear in dashboards?

Answer:
Because it was short-lived. Prometheus scrapes at intervals and may miss ephemeral pods that start and stop quickly.

üü† Advanced Level (SRE / DevOps Focus)
## Q12. Explain the Prometheus scrape model.

Answer:
Prometheus periodically scrapes targets at configured intervals. If a target is unavailable during a scrape window, its metrics are missed.

## Q13. Why is monitoring considered ‚Äúmetrics-first‚Äù?

Answer:
Metrics scale better than logs, are cheaper to store, enable alerting, and provide system-wide trends, making them the primary monitoring signal.

## Q14. How does monitoring help autoscaling?

Answer:
Autoscalers like HPA rely on metrics (CPU, memory, custom metrics) to make scaling decisions. Without metrics, autoscaling cannot function.

## Q15. What is the role of kube-state-metrics?

Answer:
kube-state-metrics exposes Kubernetes object state (deployments, replicas, pod status) as metrics, enabling insight into desired vs actual state.

## Q16. What is Alertmanager?

Answer:
Alertmanager handles alerts generated by Prometheus, including grouping, silencing, routing, and sending notifications.

## Q17. Why is monitoring not the same as observability?

Answer:
Monitoring tells you when something is wrong. Observability helps you understand why it is wrong by correlating metrics, logs, and traces.

üî¥ Scenario-Based Questions (Very Important)
## Q18. Users report slowness. How do you debug?

Answer:
Start with metrics to check CPU, memory, and latency trends. If metrics indicate issues, use logs for error details and traces for request flow.

## Q19. Autoscaling happened but performance didn‚Äôt improve. Why?

Answer:
Because the bottleneck might be external (database, API, network). Autoscaling only adds capacity; it does not fix architectural bottlenecks.

## Q20. Why shouldn‚Äôt you rely only on logs?

Answer:
Logs are expensive, noisy, and difficult to analyze at scale. They lack trends and are unsuitable for alerting or autoscaling decisions.

üß† Common Monitoring Anti-Patterns

Logging everything instead of measuring
No alerts, only dashboards
Alerting on symptoms instead of causes
Assuming empty dashboards indicate failure
Treating monitoring tools as plug-and-play

üî• One-Line Interview Answers (Memorize These)

Metrics show trends, logs show details
Prometheus stores metrics, Grafana visualizes them
Empty dashboards often mean no traffic, not misconfiguration
Monitoring shows what is happening, observability explains why

üèÅ Final Takeaway

Effective monitoring is not about installing tools, but about interpreting signals correctly and acting on them.